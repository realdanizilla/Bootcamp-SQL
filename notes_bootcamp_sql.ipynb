{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Bootcamp SQL\n",
    "\n",
    "### [Excalidraw](https://app.excalidraw.com/l/8pvW6zbNUnD/3ktnOgfFeRK)\n",
    "### [github](https://github.com/lvgalvao/data-engineering-roadmap/tree/main/Bootcamp%20-%20SQL%20e%20Analytics)\n",
    "\n",
    "\n",
    "## Aula 1\n",
    "\n",
    "Por que Postgres?\n",
    "- opensource - de graça\n",
    "- presente em várias empresas\n",
    "- um dos principais bancos na logica cliente-servidor\n",
    "- suporta tipos complexos como json\n",
    "\n",
    "consultar a documentação (link na aula 1 readme)\n",
    "\n",
    "Comandos principais\n",
    "- SELECT\n",
    "  - SELECT colunas ou * FROM tabela\n",
    "  - SELECT distinct coluna FROM tabela\n",
    "  - SELECT count(distinct coluna) FROM tabela\n",
    "- WHERE\n",
    "  - SELECT * FROM tabela WHERE coluna = 'valor' AND coluna2 = 'valor2'\n",
    "  - SELECT * FROM tabela WHERE coluna <> 'valor'\n",
    "  - SELECT * FROM tabela WHERE coluna = 'valor' AND (coluna2 = 'valor2' OR coluna2 = 'valor3')\n",
    "- ORDER BY\n",
    "  - SELECT * FROM tabela ORDER BY coluna1 ASC, coluna2 DESC\n",
    "  - SELECT * FROM tabela ORDER BY coluna1, coluna2\n",
    "- LIKE e IN (é pesado, poderia concatenar varias colunas em um e dar like nessa coluna quando queremos multiplos likes. O ILIKE resolve o case senstivie to LIKE)\n",
    "  - SELECT * FROM tabela WHERE coluna LIKE 'a%'\n",
    "  - SELECT * FROM tabela WHERE coluna NOT LIKE 'a%'\n",
    "  - SELECT * FROM customers WHERE coluna IN ('valor1', 'valor2', 'valor3')\n",
    "  - SELECT * FROM customers WHERE coluna LIKE '%or%'\n",
    "  - SELECT * FROM customers WHERE coluna LIKE '%_r%' (r na segunda posição, '_' significa qualquer caractere)\n",
    "  - SELECT * FROM customers WHERE coluna LIKE 'a%o' (começa com 'a' e termina com 'o')\n",
    "  - SELECT * FROM customers WHERE coluna LIKE 'a__%' (começa com 'a' e pelo menos 3 caracteres)\n",
    "- MAIS AVANÇADOS (gostinho de subquery)\n",
    "  - SELECT * FROM customers WHERE coluna IN (SELECT DISTINCT coluna from tabela)\n",
    "  - SELECT * FROM customers WHERE coluna BETWEEN 10 AND 20 (not NOT BETWEEN)\n",
    "  - SELECT TO_CHAR(data, 'DD-MM-YYY') FROM tabela WHERE order_date between '1996-04-07' and '1996-09-07' (converte data em outro formato)\n",
    "- Agregadores\n",
    "  - MIN, MAX, AVG, COUNT, SUM - normalmente usadas com GROUP BY\n",
    "  - LIMIT # (limita a qtde de resultados)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Aula 2\n",
    "\n",
    "Podemos ir no PGAdmin e dentro do banco na parte de Login/Group Roles podemos definir permissões de acesso\n",
    "\n",
    "- SELECT é usado para consultar dados\n",
    "- INSERT, UPDATE E DELETE normalmente sao mais usados por engenheiros de dados e principalmente sistemas com esse tipo de acesso (por exemplo CRUD)\n",
    "- CREATE, ALTER, DROP sao mais usados por DBA e modeladores do banco\n",
    "- GRANT, REVOKE tem a ver com acesso, usado por admin do banco - tira, dá ou deleta conta\n",
    "- BEGIN, COMMIT, ROLLBACK tem a ver com transações. porém transações sempre sao um conjunto de comandos. Porém para garantir a atomicidade, colocamos os statements entre um BEGIN e um COMMIT para garantir que tudo que está entre eles rode. Caso algo dê errado no entre BEGIN e COMMIT, a transação nao acontece. Se der um erro eu posso usar o ROLLBACK para voltar, como se fosse um try/except do python\n",
    "\n",
    "Sempre trazer as colunas que precisamos, evitar usar o SELECT * pq pressiona a performance\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Aula 3\n",
    "\n",
    "importante a sequencia de comandos em SQL:\n",
    "1. select\n",
    "2. from\n",
    "3. join\n",
    "4. where\n",
    "5. group by\n",
    "6. having\n",
    "7. order by\n",
    "8. limit\n",
    "\n",
    "## Joins\n",
    "\n",
    "- Inner (padrão)\n",
    "- left \n",
    "- right\n",
    "- full\n",
    "- cross (menos utilizada)\n",
    "\n",
    "Se estamos usando funções agregadoras no select, o que nao está sendo agregado precisa estar no group by"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Aula 4 - Window functions\n",
    "\n",
    "Como se criasse nova coluna, similar a usar um dataframe no pandas\n",
    "\n",
    "Sintaxe:\n",
    "window_function_name(arg1, arg2, ...) OVER (\n",
    "  [PARTITION BY partition_expression, ...]\n",
    "  [ORDER BY sort_expression [ASC | DESC], ...]\n",
    ")\n",
    "\n",
    "windows function permite atingir o mesmo resultado com statements diferentes. Exemplo do Northwind:\n",
    "\n",
    "```sql\n",
    "SELECT order_id,\n",
    "       COUNT(order_id) AS unique_product,\n",
    "       SUM(quantity) AS total_quantity,\n",
    "       SUM(unit_price * quantity) AS total_price\n",
    "FROM order_details\n",
    "GROUP BY order_id\n",
    "ORDER BY order_id;\n",
    "\n",
    "-- é equivalente a \n",
    "\n",
    "SELECT DISTINCT order_id,\n",
    "   COUNT(order_id) OVER (PARTITION BY order_id) AS unique_product,\n",
    "   SUM(quantity) OVER (PARTITION BY order_id) AS total_quantity,\n",
    "   SUM(unit_price * quantity) OVER (PARTITION BY order_id) AS total_price\n",
    "FROM order_details\n",
    "ORDER BY order_id;\n",
    "```\n",
    "\n",
    "Windows function basicamente substitui o group by porém é feito ao nível de linha e o OVER + PARTITION BY deve ser incorporado a todas as colunas selecionadas. \n",
    "\n",
    "POdemos inclusive fazer o partition by para colunas diferentes gerando agrupamentos diferentes por linha\n",
    "\n",
    "É possível ter mais de uma coluna em um partition by, o que pode ser útil para algumas análises\n",
    "\n",
    "A boa prática é já trazer o dado do SQL do jeito que vc precisa, vc pode até trazer o dado bruto e tratar em outro local como um pandas, porém estará consumindo mais memória. Ex: \n",
    "- se vc joga um select * from e manda pro powerbi vai ficar lento e vai ter q ter cálculo no pbi, que não é o ideal.\n",
    "- O mesmo vale para regra de negócio colocada no front end que não é o ideal, como calcular um desconto no frontend ao invés de ter isso já calculado no banco\n",
    "\n",
    "Em termos de performance podemos usar EXPLAIN ou EXPLAIN ANALYZE antes da query para ver a performance\n",
    "\n",
    "RANK, DENSE e ROWNUMBER servem para fazer ordenação com base em alguma coisa:\n",
    "- ROWNUMBER nao tem repetido, ele dá um jeito de desempatar, usa uma coluna ID implícita para ordenar, é não-determinístico em caso de empate\n",
    "- RANK tem repetido, nao desempata e ordena normalmente, entao se tem 2 em 1o, o próximo será 3o         \n",
    "- DENSE tem repetido, nao desempata e ordena pulando, entao se tem 2 em 1o, o próximo será 2o         \n",
    "\n",
    "PERCENT_RANK() e CUME_DIST() sao menos utilizados mas podem ser úteis quando queremos ver % dentro de grupos\n",
    "\n",
    "NTILE() é usada para dividir o conjunto de resultados em um número especificado de partes aproximadamente iguais ou \"faixas\" e atribuir um número de grupo ou \"bucket\" a cada linha com base em sua posição dentro do conjunto de resultados ordenado.\n",
    "\n",
    "LAG() e LEAD() serve para comparar e ver evoluções / variações"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Exercícios Aula 4 (chat-gpt/Northwind)\n",
    "\n",
    "Group 1: RANK, DENSE_RANK, ROW_NUMBER\n",
    "Exercises:\n",
    "1. Rank orders by their total value (descending) for each customer. Include ties.\n",
    "\n",
    "```sql\n",
    "SELECT\n",
    "\tc.company_name,\n",
    "\to.order_id,\n",
    "\tSUM(od.unit_price * od.quantity) AS order_total,\n",
    "\tRANK() OVER(PARTITION BY c.company_name ORDER BY SUM(od.unit_price * od.quantity) DESC) AS order_rank\n",
    "FROM\n",
    "\tcustomers c\n",
    "JOIN\n",
    "\torders o on c.customer_id = o.customer_id\n",
    "JOIN\n",
    "\torder_details od on o.order_id = od.order_id\n",
    "GROUP BY\n",
    "\tc.company_name, o.order_id, od.unit_price, od.quantity\n",
    "```\n",
    "\n",
    "\n",
    "2. Rank products (DENSE_RANK) based on the number of orders placed, grouped by category.\n",
    "\n",
    "```sql\n",
    "SELECT\n",
    "\tp.product_name,\n",
    "\tc.category_name as category,\n",
    "\tCOUNT(od.order_id) AS order_qty,\n",
    "\tDENSE_RANK() OVER(PARTITION BY c.category_name ORDER BY COUNT(od.order_id) DESC) AS order_dense\n",
    "FROM\n",
    "\tproducts p\n",
    "JOIN\n",
    "\tcategories c on p.category_id = c.category_id\n",
    "JOIN\n",
    "\torder_details od on p.product_id = od.product_id\n",
    "GROUP BY\n",
    "\tp.product_name, c.category_name\n",
    "```\n",
    "\n",
    "3. Assign a row number (ROW_NUMBER) to each employee within their respective city, ordered by their hire date.\n",
    "4. Rank employees by their sales amount. Use RANK to ensure ties are ranked the same and leave gaps in ranks.\n",
    "5. Assign ROW_NUMBER to products within each supplier, ordered by price.\n",
    "\n",
    "\n",
    "Group 2: PERCENT_RANK, CUME_DIST\n",
    "Exercises:\n",
    "1. Calculate the PERCENT_RANK of each order based on its total value, grouped by customer.\n",
    "```sql\n",
    "SELECT\n",
    "\tc.company_name,\n",
    "\to.order_id,\n",
    "\tSUM(od.unit_price * od.quantity) AS total_value,\n",
    "\tROUND(CAST(PERCENT_RANK() OVER (PARTITION BY o.order_id ORDER BY (od.unit_price * od.quantity) DESC) AS numeric),2) AS order_rank\n",
    "FROM\n",
    "\tcustomers c\n",
    "JOIN\n",
    "\torders o ON o.customer_id = c.customer_id\n",
    "JOIN \n",
    "\torder_details od ON od.order_id = o.order_id\n",
    "GROUP BY c.company_name, o.order_id, od.unit_price, od.quantity\n",
    "```\n",
    "2. Compute the PERCENT_RANK of employees based on their hire date (earliest hire date = 0, latest = 1).\n",
    "3. Calculate the CUME_DIST of orders based on their total value across all orders (not grouped).\n",
    "4. Determine the CUME_DIST of products based on their price, grouped by category.\n",
    "```sql\n",
    "SELECT\n",
    "\tc.category_name,\n",
    "\tp.product_name,\n",
    "\tp.unit_price,\t\n",
    "\tROUND(CAST(CUME_DIST() OVER (partition by c.category_name ORDER BY p.unit_price DESC) AS numeric),4) as cum_percent\n",
    "FROM\n",
    "\tproducts p\n",
    "JOIN\n",
    "\tcategories c on c.category_id = p.category_id\n",
    "\t\n",
    "GROUP BY \n",
    "\tp.product_name, p.unit_price\n",
    "```\n",
    "5. For each employee, compute the PERCENT_RANK of their total sales.\n",
    "\n",
    "Group 3: NTILE\n",
    "Exercises:\n",
    "1. Divide all orders into 4 equal-sized groups (quartiles) based on their total value.\n",
    "```sql\n",
    "SELECT\n",
    "\tod.order_id,\n",
    "\tROUND(CAST(SUM(od.unit_price * od.quantity) AS NUMERIC),0) AS total_value,\n",
    "\tNTILE(4) OVER (ORDER BY SUM(od.unit_price * od.quantity) DESC) AS ntile_rank\n",
    "FROM\n",
    "\torder_details od\n",
    "GROUP BY \n",
    "\tod.order_id\n",
    "```\n",
    "2. Divide employees into 5 NTILE groups based on their hire date, with the earliest hire date in NTILE 1.\n",
    "3. Categorize products into 3 NTILE groups within each category, based on their unit price.\n",
    "4. Partition orders by year and assign each order to 4 NTILE groups based on their total value within that year.\n",
    "```sql\n",
    "SELECT\n",
    "\tod.order_id,\n",
    "\tEXTRACT(YEAR FROM o.order_date) as order_year,\n",
    "\tROUND(CAST(SUM(od.unit_price * od.quantity) AS NUMERIC),0) AS total_value,\n",
    "\tNTILE(4) OVER (PARTITION BY EXTRACT(YEAR FROM o.order_date) ORDER BY SUM(od.unit_price * od.quantity) DESC) AS ntile_rank\n",
    "FROM\n",
    "\torder_details od\n",
    "JOIN\n",
    "\torders o on o.order_id = od.order_id\n",
    "GROUP BY \n",
    "\tod.order_id, o.order_date\n",
    "```\n",
    "5. Divide customers into 10 NTILE groups based on the total number of orders placed by each.\n",
    "\n",
    "Group 4: LAG, LEAD\n",
    "Exercises:\n",
    "1. For each order, use LAG to calculate the time difference (in days) between the current order and the previous order for the same customer.\n",
    "```sql\n",
    "SELECT\n",
    "\to.order_id,\n",
    "\tc.company_name,\n",
    "\to.order_date,\n",
    "\to.order_date - LAG(o.order_date) OVER (PARTITION BY o.customer_id ORDER BY o.order_date) AS day_diff\n",
    "FROM\n",
    "\torders o\n",
    "JOIN\n",
    "\tcustomers c on c.customer_id = o.customer_id\n",
    "```\n",
    "2. For each product, use LEAD to display the unit price of the next product within the same category.\n",
    "3. For each employee, compute their sales amount and show the sales amount of the previous employee using LAG.\n",
    "4. For each order, display the next order's total value for the same customer using LEAD.\n",
    "```sql\n",
    "SELECT \n",
    "    orders.customer_id,\n",
    "    orders.order_id,\n",
    "\torders.order_date,\n",
    "    ROUND(CAST(SUM(od.unit_price * od.quantity) AS numeric),2) AS total_value,\n",
    "    ROUND(CAST(LEAD(SUM(od.unit_price * od.quantity)) OVER (PARTITION BY orders.customer_id ORDER BY orders.order_id ASC) AS numeric),2) AS NextOrderValue\n",
    "FROM\n",
    "\torders\n",
    "JOIN\n",
    "\torder_details od ON od.order_id = orders.order_id\n",
    "GROUP BY\n",
    "\torders.customer_id, orders.order_id\n",
    "```\n",
    "5. For each product, calculate the difference between its unit price and the unit price of the next product in the same category using LEAD."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Aula 5 - Projeto SQL\n",
    "\n",
    "Dicas para teste técnico:\n",
    "- Entregar testes técnicos como um repositório git bem organizado\n",
    "- Colocar as queries no readme do repositório e também uma por arquivo .sql em uma pastinha no repositório\n",
    "- Colocar contexto (explicação sobre o que é o banco)\n",
    "- Configuração inicial / como usar e instalar o repositório\n",
    "- Colocar tudo em um docker container e setar o docker compose"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Aula 5 desafio\n",
    "\n",
    "Precisamos pensar na performance da query, pois duas queries que geram o mesmo resultado podem ter performances diferentes. ex:\n",
    "\n",
    "Consulta 1 (menos performática pois o join é com a tabela completa)\n",
    "```sql\n",
    "SELECT \n",
    "\tROUND(CAST(SUM((od.unit_price * od.quantity) * (1.0 - od.discount)) AS numeric),0) AS total_net_sales\n",
    "FROM\n",
    "\torder_details od\n",
    "JOIN\n",
    "\torders o ON o.order_id = od.order_id\n",
    "WHERE\n",
    "\tEXTRACT(YEAR FROM o.order_date) = '1997'\n",
    "```\n",
    "\n",
    "Consulta 2 (mais performática, pq gera uma consulta só com ano de 1997 antes de fazer o join)\n",
    "\n",
    "```sql\n",
    "SELECT SUM((order_details.unit_price) * order_details.quantity * (1.0 - order_details.discount)) AS total_revenues_1997\n",
    "FROM order_details\n",
    "INNER JOIN (\n",
    "    SELECT order_id \n",
    "    FROM orders \n",
    "    WHERE EXTRACT(YEAR FROM order_date) = '1997'\n",
    ") AS ord \n",
    "ON ord.order_id = order_details.order_id;\n",
    "```\n",
    "\n",
    "Nao exist WHERE e HAVING de CTE, precisa rodar uma SELECT com WITH e depois fazer um SELECT com WHERE\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "vscode": {
     "languageId": "sql"
    }
   },
   "outputs": [],
   "source": [
    "-- Qual foi o total de receitas no ano de 1997?\n",
    "\n",
    "CREATE VIEW total_revenue_1997 AS\n",
    "SELECT \n",
    "\tROUND(CAST(SUM((od.unit_price * od.quantity) * (1.0 - od.discount)) AS numeric),0) AS total_net_sales\n",
    "FROM\n",
    "\torder_details od\n",
    "JOIN\n",
    "\torders o ON o.order_id = od.order_id\n",
    "WHERE\n",
    "\tEXTRACT(YEAR FROM o.order_date) = '1997'\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "vscode": {
     "languageId": "sql"
    }
   },
   "outputs": [],
   "source": [
    "-- Faça uma análise de crescimento mensal e o cálculo de YTD\n",
    "\n",
    "CREATE VIEW month_ytd_growth AS \n",
    "WITH ReceitasMensais AS (\n",
    "\tSELECT\n",
    "\t\tEXTRACT(YEAR FROM o.order_date) as order_year,\n",
    "\t\tEXTRACT(MONTH FROM o.order_date) as order_month,\n",
    "\t\tROUND(CAST(SUM((od.unit_price * od.quantity) * (1.0 - od.discount)) AS numeric),0) AS month_revenue\n",
    "\tFROM\n",
    "\t\torders o\n",
    "\tJOIN\n",
    "\t\torder_details od on od.order_id = o.order_id\n",
    "\tGROUP BY\n",
    "\t\tEXTRACT(YEAR FROM o.order_date),\n",
    "\t\tEXTRACT(MONTH FROM o.order_date)\n",
    "\t), \n",
    "\tReceitasAcumuladas AS (\n",
    "\t\tSELECT \n",
    "\t\t\torder_year,\n",
    "\t\t \torder_month,\n",
    "\t\t\tmonth_revenue,\n",
    "\t\t\tSUM(month_revenue) OVER (PARTITION BY order_year ORDER BY order_month) as receita_ytd\n",
    "\t\tFROM\n",
    "\t\t\tReceitasMensais\n",
    "\t)\n",
    "\tSELECT\n",
    "\t\torder_year,\n",
    "\t\torder_month,\n",
    "\t\tmonth_revenue,\n",
    "\t\tmonth_revenue - LAG(month_revenue) OVER (PARTITION BY order_year ORDER BY order_month) AS month_diff,\n",
    "\t\treceita_ytd,\n",
    "\t\tROUND(\n",
    "\t\t\tCAST(\n",
    "\t\t\t\t(month_revenue - LAG(month_revenue) \n",
    "\t\t\t\tOVER (PARTITION BY order_year ORDER BY order_month)) / \n",
    "\t\t\t\tLAG(month_revenue) \n",
    "\t\t\t\tOVER (PARTITION BY order_year ORDER BY order_month) * 100 AS numeric),2) AS pct_month_change\n",
    "\tFROM \n",
    "\t\tReceitasAcumuladas\n",
    "\tORDER BY\n",
    "\t\torder_year,\n",
    "\t\torder_month\n",
    "\t"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "vscode": {
     "languageId": "sql"
    }
   },
   "outputs": [],
   "source": [
    "-- Segmentação de clientes. Qual é o valor total que cada cliente já pagou até agora?\n",
    "\n",
    "CREATE VIEW net_sales_per_customer AS\n",
    "SELECT\n",
    "\tc.company_name,\n",
    "\tROUND(CAST(SUM((od.unit_price * od.quantity) * (1 - od.discount)) AS numeric),0) as net_sales\n",
    "FROM\n",
    "\tcustomers c\n",
    "JOIN\n",
    "\torders o ON o.customer_id = c.customer_id\n",
    "JOIN\n",
    "\torder_details od ON od.order_id = o.order_id\n",
    "GROUP BY\n",
    "\tc.company_name\n",
    "ORDER BY\n",
    "\tSUM((od.unit_price * od.quantity) * (1 - od.discount)) DESC"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "vscode": {
     "languageId": "sql"
    }
   },
   "outputs": [],
   "source": [
    "-- Separe os clientes em 5 grupos de acordo com o valor pago por cliente\n",
    "\n",
    "CREATE VIEW customer_groups AS\n",
    "SELECT\n",
    "\tc.company_name,\n",
    "\tROUND(CAST(SUM((od.unit_price * od.quantity) * (1 - od.discount)) AS numeric),0) as net_sales,\n",
    "\tNTILE(5) OVER (ORDER BY SUM((od.unit_price * od.quantity) * (1 - od.discount)) DESC) as ntile_group\n",
    "FROM\n",
    "\tcustomers c\n",
    "JOIN\n",
    "\torders o ON o.customer_id = c.customer_id\n",
    "JOIN\n",
    "\torder_details od ON od.order_id = o.order_id\n",
    "GROUP BY\n",
    "\tc.company_name\n",
    "ORDER BY\n",
    "\tSUM((od.unit_price * od.quantity) * (1 - od.discount)) DESC"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "vscode": {
     "languageId": "sql"
    }
   },
   "outputs": [],
   "source": [
    "-- Agora somente os clientes que estão nos grupos 3, 4 e 5 para que seja feita uma análise de Marketing especial com eles\n",
    "\n",
    "\n",
    "CREATE VIEW customer_groups_123 AS\n",
    "WITH marketing_customers AS(\n",
    "\tSELECT\n",
    "\t\tc.company_name,\n",
    "\t\tROUND(CAST(SUM((od.unit_price * od.quantity) * (1 - od.discount)) AS numeric),0) as net_sales,\n",
    "\t\tNTILE(5) OVER (ORDER BY SUM((od.unit_price * od.quantity) * (1 - od.discount)) DESC) as ntile_group\n",
    "\tFROM\n",
    "\t\tcustomers c\n",
    "\tJOIN\n",
    "\t\torders o ON o.customer_id = c.customer_id\n",
    "\tJOIN\n",
    "\t\torder_details od ON od.order_id = o.order_id\n",
    "\tGROUP BY\n",
    "\t\tc.company_name\n",
    "\tORDER BY\n",
    "\t\tSUM((od.unit_price * od.quantity) * (1 - od.discount)) DESC\n",
    ")\n",
    "SELECT * \n",
    "FROM marketing_customers\n",
    "WHERE ntile_group <= 3"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "vscode": {
     "languageId": "sql"
    }
   },
   "outputs": [],
   "source": [
    "-- Top 10 Produtos Mais VendidosIdentificar os 10 produtos mais vendidos.. \n",
    "\n",
    "CREATE VIEW most_revenue_products AS\n",
    "SELECT\n",
    "\tp.product_name,\n",
    "\tROUND(CAST(SUM((od.unit_price * od.quantity) * (1 - od.discount)) AS numeric),0) as net_sales\n",
    "FROM\n",
    "\tproducts p\n",
    "JOIN\n",
    "\torder_details od ON od.product_id = p.product_id\n",
    "GROUP BY\n",
    "\tp.product_name\n",
    "ORDER BY\n",
    "\tSUM((od.unit_price * od.quantity) * (1 - od.discount)) DESC\n",
    "LIMIT 10"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "vscode": {
     "languageId": "sql"
    }
   },
   "outputs": [
    {
     "ename": "SyntaxError",
     "evalue": "invalid syntax (4052326442.py, line 2)",
     "output_type": "error",
     "traceback": [
      "\u001b[1;36m  Cell \u001b[1;32mIn[1], line 2\u001b[1;36m\u001b[0m\n",
      "\u001b[1;33m    CREATE VIEW uk_customers_above1000 AS\u001b[0m\n",
      "\u001b[1;37m           ^\u001b[0m\n",
      "\u001b[1;31mSyntaxError\u001b[0m\u001b[1;31m:\u001b[0m invalid syntax\n"
     ]
    }
   ],
   "source": [
    "-- Clientes do Reino Unido que Pagaram Mais de 1000 Dólares. Quais clientes do Reino Unido pagaram mais de 1000 dólares?\n",
    "CREATE VIEW uk_customers_above1000 AS\n",
    "SELECT\n",
    "\tc.company_name,\n",
    "\tROUND(CAST(SUM((od.unit_price * od.quantity) * (1 - od.discount)) AS numeric),0) AS net_sales\n",
    "FROM\n",
    "\tcustomers c\n",
    "JOIN\n",
    "\torders o ON o.customer_id = c.customer_id\n",
    "JOIN\n",
    "\torder_details od on od.order_id = o.order_id\n",
    "WHERE \n",
    "\tc.country = 'UK'\n",
    "GROUP BY\n",
    "\tc.company_name\n",
    "HAVING\n",
    "\tROUND(CAST(SUM((od.unit_price * od.quantity) * (1 - od.discount)) AS numeric),0) > 1000"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Aula 6\n",
    "\n",
    "## CTE, Subquery, View e View materializada\n",
    "\n",
    "Here’s a detailed comparison of **SQL CTEs (Common Table Expressions)**, **Subqueries**, **Views**, **Temporary Tables**, and **Materialized Views** based on their similarities, differences, advantages, disadvantages, and real-life use cases.\n",
    "\n",
    "---\n",
    "\n",
    "## **1. Overview**\n",
    "\n",
    "| **Feature**         | **Description**                                                                                           |\n",
    "|---------------------|-----------------------------------------------------------------------------------------------------------|\n",
    "| **CTEs**            | Temporary, named result sets used within a single query. Declared using `WITH`.                           |\n",
    "| **Subqueries**      | Queries nested inside another query (can appear in `SELECT`, `FROM`, or `WHERE` clauses).                 |\n",
    "| **Views**           | Named, reusable SQL queries stored in the database. Executes every time it is referenced.                 |\n",
    "| **Temporary Tables**| Tables that exist only for the duration of a session or transaction.                                      |\n",
    "| **Materialized Views**| Precomputed, stored result sets that can be refreshed on demand or periodically.                         |\n",
    "\n",
    "---\n",
    "\n",
    "## **2. Similarities**\n",
    "\n",
    "1. **Reusable Logic**:\n",
    "   - All allow breaking complex queries into simpler components.\n",
    "\n",
    "2. **Encapsulation**:\n",
    "   - Each provides a way to encapsulate SQL logic, improving readability and maintainability.\n",
    "\n",
    "3. **Integration**:\n",
    "   - Can be used in conjunction with joins, aggregations, and filtering in complex queries.\n",
    "\n",
    "---\n",
    "\n",
    "## **3. Differences**\n",
    "\n",
    "| **Feature**          | **Scope**                   | **Execution**                                    | **Persistence**                      | **Performance**                          |\n",
    "|----------------------|-----------------------------|------------------------------------------------|--------------------------------------|------------------------------------------|\n",
    "| **CTEs**             | Query scope (temporary).    | Re-evaluated for each query execution.          | Not stored; temporary to the query.  | Good for breaking down queries but slower for reuse. |\n",
    "| **Subqueries**       | Query scope (temporary).    | Re-evaluated for each query execution.          | Not stored; inline within query.     | Inline execution; less readable for complex queries. |\n",
    "| **Views**            | Database scope (persistent).| Re-evaluated each time it is queried.           | Stored in the database.              | Depends on the query; no precomputation.             |\n",
    "| **Temporary Tables** | Session or transaction.     | Evaluated once when created.                    | Exists for the session/transaction.  | Optimized for reuse during the session.              |\n",
    "| **Materialized Views**| Database scope (persistent).| Query results precomputed and stored.           | Stored in the database.              | Faster for repeated use; requires maintenance to refresh. |\n",
    "\n",
    "---\n",
    "\n",
    "## **4. Advantages and Disadvantages**\n",
    "\n",
    "### **CTEs**\n",
    "- **Advantages**:\n",
    "  - Simplifies query readability by breaking queries into smaller parts.\n",
    "  - Supports recursion (e.g., hierarchical data).\n",
    "  - Temporary; does not clutter database.\n",
    "- **Disadvantages**:\n",
    "  - Recomputed every time the query runs, potentially reducing performance for large datasets or frequent reuse.\n",
    "\n",
    "---\n",
    "\n",
    "### **Subqueries**\n",
    "- **Advantages**:\n",
    "  - Inline, self-contained, and requires no external definitions.\n",
    "  - Works well for one-off operations.\n",
    "- **Disadvantages**:\n",
    "  - Hard to read and debug when deeply nested.\n",
    "  - Cannot reuse logic; duplication is required for similar subqueries.\n",
    "\n",
    "---\n",
    "\n",
    "### **Views**\n",
    "- **Advantages**:\n",
    "  - Centralized logic: Simplifies reuse across applications and queries.\n",
    "  - Improves code maintainability.\n",
    "  - Abstracts database complexity for end users.\n",
    "- **Disadvantages**:\n",
    "  - Performance depends on the underlying query and database engine.\n",
    "  - Cannot store precomputed results; always executes the underlying query.\n",
    "\n",
    "---\n",
    "\n",
    "### **Temporary Tables**\n",
    "- **Advantages**:\n",
    "  - Ideal for intermediate calculations and large transformations.\n",
    "  - Can be indexed for better performance during the session.\n",
    "- **Disadvantages**:\n",
    "  - Requires manual cleanup if used improperly.\n",
    "  - Adds overhead due to creation and deletion during the session.\n",
    "\n",
    "---\n",
    "\n",
    "### **Materialized Views**\n",
    "- **Advantages**:\n",
    "  - Significant performance boost for repeated queries.\n",
    "  - Precomputed results reduce computation overhead.\n",
    "  - Can be refreshed periodically or on-demand.\n",
    "- **Disadvantages**:\n",
    "  - Requires storage space for precomputed results.\n",
    "  - Must be refreshed to reflect updated underlying data.\n",
    "\n",
    "---\n",
    "\n",
    "## **5. Real-Life Use Cases**\n",
    "\n",
    "| **Feature**          | **Use Case**                                                                                                                                          |\n",
    "|----------------------|-------------------------------------------------------------------------------------------------------------------------------------------------------|\n",
    "| **CTEs**             | - Simplifying a complex query for sales trend analysis.                                                                                             |\n",
    "|                      | - Implementing recursive queries to manage hierarchical data like organizational structures.                                                         |\n",
    "| **Subqueries**       | - Filtering rows based on aggregate conditions (e.g., products with sales above average).                                                           |\n",
    "|                      | - Dynamically calculating derived values, such as ranks, inline.                                                                                    |\n",
    "| **Views**            | - Centralized reusable queries for analytics dashboards (e.g., monthly revenue report).                                                             |\n",
    "|                      | - Abstracting joins and complex relationships for end-user query interfaces.                                                                         |\n",
    "| **Temporary Tables** | - Intermediate calculations during ETL processes.                                                                                                   |\n",
    "|                      | - Complex report generation that requires multiple transformations.                                                                                 |\n",
    "| **Materialized Views**| - Frequently accessed summary reports (e.g., total revenue by region).                                                                              |\n",
    "|                      | - Precomputing and storing aggregates for large datasets like time-series data to speed up queries.                                                 |\n",
    "\n",
    "---\n",
    "\n",
    "## **6. When to Use Each**\n",
    "\n",
    "| **Feature**           | **When to Use**                                                                                               |\n",
    "|-----------------------|-------------------------------------------------------------------------------------------------------------|\n",
    "| **CTEs**              | Use when you need to improve query readability or work with temporary, reusable logic within a single query. |\n",
    "| **Subqueries**        | Use for quick, inline operations that don't need reuse or external storage.                                  |\n",
    "| **Views**             | Use when you need reusable logic across multiple queries and want to abstract database complexity.           |\n",
    "| **Temporary Tables**  | Use for intermediate steps in a process that requires multiple queries within a session or transaction.      |\n",
    "| **Materialized Views**| Use for performance-critical queries that require precomputed and reusable results.                          |\n",
    "\n",
    "---\n",
    "\n",
    "### Example Scenarios with SQL Code\n",
    "\n",
    "#### **CTE Example**\n",
    "Find employees with salaries above the department average:\n",
    "```sql\n",
    "WITH DeptAvg AS (\n",
    "    SELECT DepartmentID, AVG(Salary) AS AvgSalary\n",
    "    FROM Employees\n",
    "    GROUP BY DepartmentID\n",
    ")\n",
    "SELECT e.EmployeeID, e.Salary\n",
    "FROM Employees e\n",
    "JOIN DeptAvg d ON e.DepartmentID = d.DepartmentID\n",
    "WHERE e.Salary > d.AvgSalary;\n",
    "```\n",
    "\n",
    "---\n",
    "\n",
    "#### **Subquery Example**\n",
    "Find products with prices above the average price:\n",
    "```sql\n",
    "SELECT ProductID, ProductName\n",
    "FROM Products\n",
    "WHERE UnitPrice > (SELECT AVG(UnitPrice) FROM Products);\n",
    "```\n",
    "\n",
    "---\n",
    "\n",
    "#### **View Example**\n",
    "Create a view for total sales by category:\n",
    "```sql\n",
    "CREATE VIEW TotalSalesByCategory AS\n",
    "SELECT c.CategoryName, SUM(od.Quantity * od.UnitPrice) AS TotalSales\n",
    "FROM Categories c\n",
    "JOIN Products p ON c.CategoryID = p.CategoryID\n",
    "JOIN OrderDetails od ON p.ProductID = od.ProductID\n",
    "GROUP BY c.CategoryName;\n",
    "```\n",
    "Query the view:\n",
    "```sql\n",
    "SELECT * FROM TotalSalesByCategory;\n",
    "```\n",
    "\n",
    "---\n",
    "\n",
    "#### **Temporary Table Example**\n",
    "Store intermediate sales data:\n",
    "```sql\n",
    "CREATE TEMP TABLE SalesTemp AS\n",
    "SELECT CustomerID, SUM(TotalAmount) AS TotalSpent\n",
    "FROM Orders\n",
    "GROUP BY CustomerID;\n",
    "\n",
    "SELECT * FROM SalesTemp WHERE TotalSpent > 1000;\n",
    "```\n",
    "\n",
    "---\n",
    "\n",
    "#### **Materialized View Example**\n",
    "Precompute total sales by product:\n",
    "```sql\n",
    "CREATE MATERIALIZED VIEW TotalSalesByProduct AS\n",
    "SELECT ProductID, SUM(Quantity * UnitPrice) AS TotalSales\n",
    "FROM OrderDetails\n",
    "GROUP BY ProductID;\n",
    "\n",
    "-- Refresh the materialized view:\n",
    "REFRESH MATERIALIZED VIEW TotalSalesByProduct;\n",
    "\n",
    "-- Query the materialized view:\n",
    "SELECT * FROM TotalSalesByProduct;\n",
    "```\n",
    "\n",
    "---\n",
    "\n",
    "### **Summary Table**\n",
    "\n",
    "| **Feature**            | **Best For**                          | **Reusable** | **Performance**           | **Persistent** | **Ideal For Large Datasets** |\n",
    "|------------------------|---------------------------------------|--------------|---------------------------|----------------|-----------------------------|\n",
    "| **CTEs**               | Readability, recursion               | No           | Slower (re-evaluates)     | No             | No                          |\n",
    "| **Subqueries**         | Inline calculations                  | No           | Slower (re-evaluates)     | No             | No                          |\n",
    "| **Views**              | Reusable queries                     | Yes          | Moderate                  | Yes            | No                          |\n",
    "| **Temporary Tables**   | Intermediate storage during sessions | No           | Moderate to fast          | No             | Yes                         |\n",
    "| **Materialized Views** | Precomputed reusable results         | Yes          | Fast (precomputed)        | Yes            | Yes                         |\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Quick guide for SQL objects\n",
    "\n",
    "[link](https://www.linkedin.com/feed/update/urn:li:activity:7190722950499577856/)\n",
    "\n",
    "1. Sua análise é simples e direta? Se sim use query, senao avance\n",
    "2. A análise necessita de  interacoes com multiplas tabelas ou dados relacionados? se sim use subquery, senao avance\n",
    "3. A consulta é para uma análise única e complexa mas vc quer manter legibilidade? se sim use CTE, senao avance\n",
    "4. Voce está lidando com uma analise ao longo do dia e quer  manter a estrutura organizada sem afetar o esquema principal? se sim use TempView, senao avance\n",
    "5. Voce está criando relatórios ou dashboards que seráo utilizados com frequencia e precisa de acessos e permissoes diferenciadas? se sim use view, senao avance\n",
    "6. voce está tentando novas consultas como estratégia de index ou precisa armazenar dados por um curto período? se sim use TempTable, senao avance\n",
    "7. Voce precisa armazenar informacoes a longo prazo? Elas serao usadas pela aplicacao principal para CRUD? se sim use tabela, senao avance\n",
    "8. Os dados serao usados por milhoes de usuarios (ex black friday)? Se sim use estrutura de cache do redis\n",
    "9. Essa análise sera feita frequentemente e envolve qtde massiva de dados? se sim, migrar para BD OLAP\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Quando lidamos com codigo / banco antigos as vezes perdemos os indices quando usamos CTE mas nao quando usamos subquery\n",
    "\n",
    "CTE recursivo serve mais quando estamos lidando com hierarquias em contextos especificos\n",
    "\n",
    "A view guarda o metadado que eh a query, porem nem CTE, nem subquery e nem view guardam os dados em si. A ocupacao de memoria da view eh desprezivel\n",
    "\n",
    "view, CTE e subquery tem a mesma performance\n",
    "\n",
    "na view podemos usar index e podemos gerenciar acesso com GRANT\n",
    "\n",
    "temp table grava o resultado entao se for chamar ela de novo na mesma sessao ela nao vai reprocessar, vai trazer direto o resultado. Vale quando vou reutilizar a query e ela demora pra rodar. entao faz a temp table e usa varias vezes\n",
    "\n",
    "tudo que eh temp morre quando encerra a sessao\n",
    "\n",
    "materialized view ou snapshot \n",
    "- guarda o metadado e o resultado alocando memoria para cada snapshot, \n",
    "- tem index, \n",
    "- parece uma tabela que eu nao posso alterar. \n",
    "- Para atualizar preciso rodar o REFRESH. \n",
    "- Vai tirando snapshots de tempos em tempos para saber como estavam os dados em determinados momentos. \n",
    "- pode ser gerada somente quando temos alteracoes nos dados\n",
    "- facilita para auditoria\n",
    "\n",
    "vacuum eh a desfragmentacao de disco\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Aula 7 e 8- Stored Procedures\n",
    "\n",
    "Stored Procedures tendem a ser preferidas por DBAs mas nao por desenvolvedores backend\n",
    "\n",
    "O que é - como se fosse uma funçao porém nao tem retorno, o objetivo é alterar coisas já existentes no banco (tabela, valores, etc) é uma rotina armazenada no banco da dados\n",
    "\n",
    "Analistas e sistemas podem invocar um stored procedure. Backend ou frontend podem consumir. Ao inves de colocar a regra na API podemos colocar no banco de dados. Tem q ver qual a melhor estratégia\n",
    "\n",
    "Se for feito stored procedure, ele fica no banco, logo está sob o domínio do DBA e nao do time de desenvolvimento ou de dados\n",
    "\n",
    "podemos gerenciar acesso no nivel da stored procedure e nao da tabela, caso conveniente\n",
    "\n",
    "quando colocamos SERIAL em uma coluna, estamos delegando ao banco fazer o sequenciamento. \n",
    "\n",
    "Podemos rodar tambem um UID que anonimiza o numero, o que é uma boa prática Para usar o UUID rodar o comando abaixo:\n",
    "\n",
    "```sql\n",
    "CREATE EXTENSION IF NOT EXISTS \"uuid-ossp\";\n",
    "\n",
    "CREATE TABLE IF NOT EXISTS clients (\n",
    "    id UUID PRIMARY KEY DEFAULT uuid_generate_v4(),\n",
    "    limite INTEGER NOT NULL,\n",
    "    saldo INTEGER NOT NULL\n",
    ");\n",
    "```\n",
    "\n",
    "O comando DELETE é similar ao SELECT em termos de sintaxe\n",
    "\n",
    "O tipo de variável CHAR em sql tem um número de caracteres definido, o VARCHAR pode variar de 1 até o numero estipulado\n",
    "\n",
    "No exemplo do banco itau precisamos de uma funcao que\n",
    "1. veja o saldo e o limite docliente\n",
    "2. compare, em caso de tipo 'd'se saldo + limite >= transacao\n",
    "3. criar a transacao\n",
    "4. atualizar o saldo \n",
    "\n",
    "Alem disso precisamos colocar checks na tabela de clients, entao quando da criacao da tabela, incluimos constraints\n",
    "\n",
    "```sql\n",
    "CREATE TABLE IF NOT EXISTS clients (\n",
    "    id UUID PRIMARY KEY DEFAULT uuid_generate_v4(),\n",
    "    limite INTEGER NOT NULL,\n",
    "    saldo INTEGER NOT NULL,\n",
    "    CHECK (saldo >= limite)\n",
    "\n",
    ");\n",
    "```\n",
    "Sempre precisamos pensar se as nossas tabelas precisam de constaints e adicionar caso necessário\n",
    "\n",
    "PAra criar stored procedures usamos CREATE OR REPLACE PROCEDURE nome (), daí passamos os parametros. É importante definir se sao parametros que entram (IN) ou que saem/extraem (OUT)\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Aula 9 Triggers\n",
    "\n",
    "observam eventos de insert into, update, delete\n",
    "\n",
    "em DB preferimos usar dados do tipo integer ou decimal(10,2), evitar ao máximo float\n",
    "\n",
    "Trigger é uma regra de negocio associada a uma tabela. Por isso nao dá pra fazer trigger em duas tabelas ao mesmo tempo\n",
    "\n",
    "POdemos ter cenarios do tipo:\n",
    "1. Store procedure atualiza title do employee, a atualizaçao do title dispara um trigger que execute uma funcao que adiciona o title antigo e o novo em uma tabela de auditoria\n",
    "2.  Criamos uma materialized view que precisa de refresh a cada atualizacao de duas outras tabelas, entao criamos 2 triggers iguais de update em cada uma das tabelas, esses triggers executam a mesma funcao que dá refresh na materialized view\n",
    "3. Star Schema - temos tabelas do tipo orders, order_details e product_details, podemos criar triggers nelas que a cada alteracao (insert, update ou delete) executamos uma funçao que faz o INSERT INTO uma tabela do tipo factVendas"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Converting the Microsoft **Northwind database** into a **Star Schema** involves reorganizing the relational database tables into a structure optimized for analytical queries and reporting. A Star Schema typically consists of a **central fact table** surrounded by **dimension tables**.\n",
    "\n",
    "Here’s how to approach the transformation step by step:\n",
    "\n",
    "---\n",
    "\n",
    "## **Step 1: Understand the Existing Northwind Schema**\n",
    "The Northwind database is a traditional OLTP (Online Transaction Processing) schema, with normalized tables such as `Orders`, `Products`, `Customers`, and `Suppliers`. Study the existing schema:\n",
    "- Identify **key tables** like `Orders`, `OrderDetails`, `Products`, and `Employees`.\n",
    "- Understand relationships:\n",
    "  - Example: `Orders` is linked to `OrderDetails`, which is linked to `Products`.\n",
    "\n",
    "---\n",
    "\n",
    "## **Step 2: Define the Fact Table**\n",
    "The **fact table** will hold the metrics you want to analyze, such as `sales`, `quantity`, or `profit`. In the Northwind context, the fact table will likely represent **sales transactions**.\n",
    "\n",
    "**Fact Table: `FactSales`**\n",
    "- **Measures**: Numeric values for analysis, e.g., `OrderQuantity`, `TotalSales`.\n",
    "- **Foreign Keys**: References to dimension tables.\n",
    "- Source tables: `OrderDetails` (for product-level order data) and `Orders` (for order metadata).\n",
    "\n",
    "**Columns**:\n",
    "| Column Name      | Description                         |\n",
    "|------------------|-------------------------------------|\n",
    "| `OrderID`        | Links to the `Orders` table.        |\n",
    "| `ProductID`      | Links to the `Products` table.      |\n",
    "| `CustomerID`     | Links to the `Customers` table.     |\n",
    "| `EmployeeID`     | Links to the `Employees` table.     |\n",
    "| `OrderDate`      | Date of the order.                  |\n",
    "| `Quantity`       | Total quantity ordered.             |\n",
    "| `UnitPrice`      | Price per unit of the product.      |\n",
    "| `TotalSales`     | Calculated as `Quantity * UnitPrice`.|\n",
    "\n",
    "---\n",
    "\n",
    "## **Step 3: Define Dimension Tables**\n",
    "Dimension tables provide context for analysis (e.g., \"Who bought the product?\" \"When was it sold?\").\n",
    "\n",
    "### **Dimension 1: `DimCustomer`**\n",
    "- **Purpose**: Store customer details.\n",
    "- **Source**: `Customers` table.\n",
    "\n",
    "**Columns**:\n",
    "| Column Name      | Description               |\n",
    "|------------------|---------------------------|\n",
    "| `CustomerID`     | Primary key.              |\n",
    "| `CustomerName`   | Name of the customer.     |\n",
    "| `Region`         | Geographic region.        |\n",
    "| `Country`        | Country of the customer.  |\n",
    "| `City`           | City of the customer.     |\n",
    "\n",
    "---\n",
    "\n",
    "### **Dimension 2: `DimProduct`**\n",
    "- **Purpose**: Store product details.\n",
    "- **Source**: `Products` table.\n",
    "\n",
    "**Columns**:\n",
    "| Column Name      | Description               |\n",
    "|------------------|---------------------------|\n",
    "| `ProductID`      | Primary key.              |\n",
    "| `ProductName`    | Name of the product.      |\n",
    "| `Category`       | Product category.         |\n",
    "| `SupplierID`     | Links to the supplier.    |\n",
    "| `UnitPrice`      | Price per unit.           |\n",
    "\n",
    "---\n",
    "\n",
    "### **Dimension 3: `DimEmployee`**\n",
    "- **Purpose**: Store employee details.\n",
    "- **Source**: `Employees` table.\n",
    "\n",
    "**Columns**:\n",
    "| Column Name      | Description               |\n",
    "|------------------|---------------------------|\n",
    "| `EmployeeID`     | Primary key.              |\n",
    "| `EmployeeName`   | Full name of the employee.|\n",
    "| `Title`          | Job title.                |\n",
    "| `Region`         | Employee's region.        |\n",
    "\n",
    "---\n",
    "\n",
    "### **Dimension 4: `DimDate`**\n",
    "- **Purpose**: Store date information for time-based analysis.\n",
    "- **Source**: Generated based on the range of dates in the `Orders` table.\n",
    "\n",
    "**Columns**:\n",
    "| Column Name      | Description                   |\n",
    "|------------------|-------------------------------|\n",
    "| `DateKey`        | Unique date identifier.       |\n",
    "| `Date`           | Actual date.                 |\n",
    "| `Year`           | Year part of the date.        |\n",
    "| `Month`          | Month part of the date.       |\n",
    "| `Day`            | Day part of the date.         |\n",
    "| `Quarter`        | Quarter part of the date.     |\n",
    "\n",
    "---\n",
    "\n",
    "## **Step 4: Create the Star Schema**\n",
    "1. **Create Fact Table**:\n",
    "   - Aggregate data from `Orders` and `OrderDetails` into the `FactSales` table.\n",
    "   - Example SQL:\n",
    "     ```sql\n",
    "     CREATE TABLE FactSales AS\n",
    "     SELECT \n",
    "         o.OrderID,\n",
    "         od.ProductID,\n",
    "         o.CustomerID,\n",
    "         o.EmployeeID,\n",
    "         o.OrderDate,\n",
    "         od.Quantity,\n",
    "         od.UnitPrice,\n",
    "         (od.Quantity * od.UnitPrice) AS TotalSales\n",
    "     FROM Orders o\n",
    "     JOIN OrderDetails od ON o.OrderID = od.OrderID;\n",
    "     ```\n",
    "\n",
    "2. **Create Dimension Tables**:\n",
    "   - Example for `DimCustomer`:\n",
    "     ```sql\n",
    "     CREATE TABLE DimCustomer AS\n",
    "     SELECT \n",
    "         CustomerID,\n",
    "         CompanyName AS CustomerName,\n",
    "         Region,\n",
    "         Country,\n",
    "         City\n",
    "     FROM Customers;\n",
    "     ```\n",
    "\n",
    "   - Example for `DimProduct`:\n",
    "     ```sql\n",
    "     CREATE TABLE DimProduct AS\n",
    "     SELECT \n",
    "         ProductID,\n",
    "         ProductName,\n",
    "         CategoryID AS Category,\n",
    "         SupplierID,\n",
    "         UnitPrice\n",
    "     FROM Products;\n",
    "     ```\n",
    "\n",
    "3. **Generate a Date Dimension**:\n",
    "   Use a date range that covers all possible `OrderDate` values:\n",
    "   ```sql\n",
    "   CREATE TABLE DimDate AS\n",
    "   SELECT \n",
    "       generate_series('1996-01-01'::date, '1998-12-31'::date, '1 day')::date AS Date,\n",
    "       EXTRACT(YEAR FROM generate_series) AS Year,\n",
    "       EXTRACT(MONTH FROM generate_series) AS Month,\n",
    "       EXTRACT(DAY FROM generate_series) AS Day,\n",
    "       EXTRACT(QUARTER FROM generate_series) AS Quarter\n",
    "   FROM generate_series('1996-01-01'::date, '1998-12-31'::date, '1 day');\n",
    "   ```\n",
    "\n",
    "---\n",
    "\n",
    "## **Step 5: Validate Relationships**\n",
    "Ensure the foreign key relationships between the fact table and dimension tables are correctly defined:\n",
    "```sql\n",
    "ALTER TABLE FactSales\n",
    "ADD CONSTRAINT fk_customer FOREIGN KEY (CustomerID) REFERENCES DimCustomer(CustomerID);\n",
    "\n",
    "ALTER TABLE FactSales\n",
    "ADD CONSTRAINT fk_product FOREIGN KEY (ProductID) REFERENCES DimProduct(ProductID);\n",
    "\n",
    "ALTER TABLE FactSales\n",
    "ADD CONSTRAINT fk_employee FOREIGN KEY (EmployeeID) REFERENCES DimEmployee(EmployeeID);\n",
    "\n",
    "ALTER TABLE FactSales\n",
    "ADD CONSTRAINT fk_date FOREIGN KEY (OrderDate) REFERENCES DimDate(Date);\n",
    "```\n",
    "\n",
    "---\n",
    "\n",
    "## **Step 6: Optimize for Reporting**\n",
    "- **Indexes**: Add indexes to frequently queried columns (e.g., `OrderDate`, `CustomerID`, etc.).\n",
    "- **Materialized Views**: For expensive aggregations, consider creating materialized views.\n",
    "- **Data Validation**: Cross-check the data between the original schema and the star schema for accuracy.\n",
    "\n",
    "---\n",
    "\n",
    "### **Summary**\n",
    "\n",
    "To transform Northwind into a Star Schema:\n",
    "1. Define the **Fact Table** (`FactSales`) with transactional data.\n",
    "2. Create **Dimension Tables** (`DimCustomer`, `DimProduct`, `DimEmployee`, `DimDate`) for entities.\n",
    "3. Populate the tables using SQL transformations.\n",
    "4. Establish **relationships** between the fact and dimension tables.\n",
    "5. Optimize for queries with indexes or materialized views.\n",
    "\n",
    "This schema will make reporting and analytical queries faster and more intuitive. Let me know if you’d like help with specific queries or implementation! 🚀"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Aula 10 - ACID\n",
    "\n",
    "Nós nao fizemos commit até agora, pq? Pq o PgAdmin tem um auto-commit habilitado por padrao. Podemos desabilitar para ter que rodar commit a cada transacao\n",
    "\n",
    "A boa prática para ter seguranca ao realizar transacoes é\n",
    "1. Desabilita o auto-commit\n",
    "2. Poe o BEGIN\n",
    "3. faz o insert, update, delete\n",
    "4. checa se ta td certo\n",
    "5. se tiver td certo, da commit\n",
    "\n",
    "Exemplo:\n",
    "```sql\n",
    "BEGIN;\n",
    "DELETE FROM tabela_exemplo; \n",
    "SELECT * FROM tabela_exemplo -- fudeu! deletei meus dados!\n",
    "ROLLBACK -- desfaz a caca\n",
    "SELECT * FROM tabela_exemplo -- dados recuperados pq nao dei commit\n",
    "```\n",
    "Executa uma linha de cada vez, selecionado e rodando cada query acima individualmente\n",
    "\n",
    "Uma transacao sempre tem um BEGIN, um bando de query que function e no final ou um commit ou um rollback. Td tem que seguir o caminho feliz para o commit funcionar, qualquer coisa que der errado já anula a transaçao (rollback)\n",
    "\n",
    "Dica de consultoria e freela - Qualquer idiota faz escopo e cronograma, o lance é fazer premissa e fora de escopo\n",
    "\n",
    "- **Atomicity** - vem do átomo (nao pode ser dividido) - a **transacao** é uma unidade de trabalho indivisivel. Ou funciona 100% ou falha 100% (ou é commit ou é rollback), nao existe parcial\n",
    "- **Isolation** - Uma transacao nao afeta a outra em diferentes sessoes, porém tem diversos níveis de isolamento e cada tecnologia de banco faz de um jeito. Ex: Um usuario dando um select em uma tabela e outro dando insert into na mesma tabela. Para vc, como analista, isolar os dados para a sua análise poderia usar\n",
    "    - SET TRANSACTION ISOLATION LEVEL REPEATABLE READ - isso congela os dados e queries abaixo desse statement \n",
    "    - Ler docs de transaction isolation no postgres`\n",
    "    - ver célula markdown abaixo com exemplo simples\n",
    "- **Consistency** - depende do isolamento, ao final da transacao mas antes de concluir, checa todos os constrain (tipo de variavel, nome de coluna, primary key, foreign key, trigger, etc) e confirma q tá tudo certo. Se deu commit é pq o dado tá lá (ou nao tá mais no caso de um delete). O commit garante a consistencia\n",
    "- **Durability** - O dado vai estar no banco daqui a 10 anos se houve um commit. O postgres tem log entao ele checa o q tá no log com os dados, tem q dar igual senao quebra\n",
    "\n",
    "Teorema CAP - posso ter 2 dos 3 abaixo, mas nao os 3:\n",
    "- Consistencia - ta todo mundo vendo a mesma coisa?\n",
    "- Availability - ta todo mundo conseguindo acessar?\n",
    "- Particionamento - mais de um banco (Particao tolerante a falhas) - A parada continua a funcionar?\n",
    "\n",
    "Ex: se quisermos alta consistencia e alta availability vamos com um banco só, porém isso náo é tolerante a falhas, se esse banco corromper já era\n",
    "\n",
    "Modelagem de banco nao é só sobre create table, tem também\n",
    "- constraint de tabela\n",
    "- triggers\n",
    "- function, index, stored procedure\n",
    "- primary key, foreign key\n",
    "- etc etc"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Alright! Let’s think of your database like a toy box, and the transactions are kids playing with the toys. Now, we want to make sure they play nicely and don’t mess up the toy box. That's where **\"transaction isolation\"** comes in—it’s like setting rules for how the kids can interact with the toys.\n",
    "\n",
    "---\n",
    "\n",
    "### **The Four Rules (Isolation Levels)**\n",
    "\n",
    "1. **Read Uncommitted**:  \n",
    "   This is like a kid peeking at another kid's toys before they finish building.  \n",
    "   - They can see half-built toys or toys being worked on (dirty reads).  \n",
    "   - In PostgreSQL, we don’t let this happen—it’s treated like the next level.\n",
    "\n",
    "---\n",
    "\n",
    "2. **Read Committed** (Default in PostgreSQL):  \n",
    "   Imagine a kid only seeing toys that are fully built and put down on the floor.  \n",
    "   - They can’t see toys someone else is still working on.  \n",
    "   - But if another kid finishes a new toy while they're looking, they might see the new toy next time.\n",
    "\n",
    "---\n",
    "\n",
    "3. **Repeatable Read**:  \n",
    "   This is like freezing time for the kid.  \n",
    "   - They only see the toys that were already built when they started playing.  \n",
    "   - No matter how many times they look around, they won’t see any new toys until they finish playing.  \n",
    "   - But there’s one tricky part: if two kids try to play with the same toy, one might have to stop and try again later.\n",
    "\n",
    "---\n",
    "\n",
    "4. **Serializable**:  \n",
    "   This is the strictest rule—it’s like only letting one kid play at a time.  \n",
    "   - Every kid takes turns, one after the other, so there’s no way for them to mess up each other’s toys.  \n",
    "   - If they try to grab the same toy, one has to start over.\n",
    "\n",
    "---\n",
    "\n",
    "### **Why These Rules Matter**\n",
    "\n",
    "- **Dirty Read**: Seeing a half-built toy = BAD!  \n",
    "- **Non-Repeatable Read**: A toy is gone when you look again = CONFUSING!  \n",
    "- **Phantom Read**: More toys appear when you look again = MAGIC (but not in a good way)!  \n",
    "- **Serialization Anomaly**: Two kids try to finish the same toy at the same time, and it breaks = CHAOS!\n",
    "\n",
    "---\n",
    "\n",
    "### **Which Rule to Use?**\n",
    "\n",
    "1. **Just Playing Quickly (Read Committed)**:  \n",
    "   If the kids don’t mind seeing only finished toys and don’t care if new ones pop up.\n",
    "\n",
    "2. **Focused Play (Repeatable Read)**:  \n",
    "   If the kids want the toys to stay the same until they’re done playing.\n",
    "\n",
    "3. **Perfect Order (Serializable)**:  \n",
    "   If the kids need everything to happen one at a time, nice and neat.\n",
    "\n",
    "---\n",
    "\n",
    "That’s it! Transaction isolation makes sure everyone plays fair and keeps the toy box (your database) tidy. Which rule you pick depends on how messy or neat you want playtime to be. 😊"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Aula 11 - Performance Tuning em SQL\n",
    "\n",
    "## Ordem de execucao e otimizacao\n",
    "\n",
    "![image.png](attachment:image.png)\n",
    "\n",
    "A imagem acima pode nao ser sempre verdade!\n",
    "\n",
    "O Postgres tem uma ordem de execucao teórica mas existe uma camada de otimizacao onde o POstgres pensa em diferentes planos de execucao que podem ter a ordem diferente da teórica, sempre pensando em velocidade, recursos disponiveis\n",
    "\n",
    "Entao um LIMIT pode melhorar a performance dependendo de varios fatores. Ele pode vir antes se fizer sentido\n",
    "\n",
    "Vale pensar tambem que isso está apenas na camada de processamento do banco, ainda tem a rede de ida e de volta q pode fazer muita diferenca em locais distantes com internet ruim"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Aula 12 B-Tree e Index\n",
    "\n",
    "Dá pra viver sem, acaba fazendo mais sentido em bancos maiores quando as queries começam a ficar mais lentas e tem mais volume de dados\n",
    "\n",
    "Índice é um objeto, constraint é um objeto, tabela, a trigger, td é objeto\n",
    "\n",
    "Quanto mais organizado mais difícil de organizar\n",
    "\n",
    "Podemos pensar na estrutura de dados bagunçada como um armário de roupas bagunçado ou como um armário profissional de loja todo arrumado - forma de organizar. **Quanto mais fácil para inserir uma peça, mais dificil para selecionar uma peça e vice-versa**\n",
    "\n",
    "POdemos pensar na diferença entre uma lista do python com qq coisa lá dentro como sendo uma estrutura de dados mais bagunçada\n",
    "\n",
    "Índice hash é similar a estrutura de dicionário do python (chave-valor)\n",
    "\n",
    "B+Tree é o mais utilizado - estrutura de árvore que pode ser ordenada ou nao. É como se fosse uma lista python, porém a cada novo número nao pode ficar galho vazio entao vai ter um reordenamento\n",
    "\n",
    "![image.png](attachment:image.png)\n",
    "\n",
    "na figura acima:\n",
    "- 10 é menor que 14, entáo vai pra esquerda\n",
    "- 10 é maior que 6, entao vai pra direita\n",
    "- achei o 10 em 3 operacoes, se fosse uma lista teria que percorrer os 36 números\n",
    "- ou seja ele vai cortando o dataset em 50% a cada checagem, reduzindo muito o tempo para alocar e encontrar os dados\n",
    "\n",
    "B-Tree é de Balanceado\n",
    "\n",
    "O que o índice de SQL é criar uma estrutra de chave-valor ordenada por B-tree. O índice é um de-para do valor do índice para os dados associados a ele. Ex: índice 7 da tabela aluno refere-se ao aluno Gustavo, 30 anos, 2 cadeiras cursando, 10 perguntas feitas na aula. Quando buscamos o índice 7 aí é que ele vai atrás dos outros dados associados ao índice 7\n",
    "\n",
    "Quando criamos um primary key, já criamos tambem automaticamente um index dessa coluna PK. Se vc fizer um select our where usando primary key, vc já tá usando index\n",
    "\n",
    "O index cria uma tabela interna com uma coluna que remete a um 'pointer' que é a referencia para encontrar o registro desejado na tabela-alvo. POrtanto cada vez q geramos um índice ele cria 3 colunas (índice + pointer e o pointer na tabela-alvo). Se todas as colunas tiverem índice, basicamente multiplicamos o custo do nosso banco por 4\n",
    "\n",
    "Deve ser feito em colunas que usamos mais como filtro e coisas mais utilizadas, deve ser usado com parcimonia\n",
    "\n",
    "Quando rodamos uma query só usando colunas que sao index, é mais performático (Ex SELECT id from pessoas WHERE id = 10000) pq o SQL só vai olhar para a tabela de índice, nem vai olhar pra nossa tabela (index only scan)\n",
    "\n",
    "Um caminho pra decidir os índices é verificar as queries mais utilizadas pelos usuários para ver o que poderia ficar mais rápido usando index. Seria um trabalho para o DBA, porém Databricks e snowflake já fazem index e partition automaticamente pra nós observando o comportamento do banco, o que pode gerar economia por nao precisar de um DBA\n",
    "- verificar onde usamos mais WHERE e GROUP BY\n",
    "- Verificar situacoes do tipo SELECT * FROM tabela where coluna LIKE '%xx%' (pior cenário ainda mais se tiver múltiplas dessas)\n",
    "\n",
    "Seq scan e table scan no explain analyze é o pior, pois ele percorreu todas as linhas. O melhor é index only scan\n",
    "\n",
    "Ordem de complexidade - Quando usamos busca normal o aumento de tempo é linear O(n), quando usamos index é log(n), entao se multiplicarmos a qtde de registros por 10, em busca normal o tempo multiplica por 10 mas a busca por indice aumenta só o dobro. Da mesma forma um algoritmo baseado em array ou list é O(n) e um baseado em b-tree é log(n)\n",
    "\n",
    "PAra criar um index em uma coluna usamos:\n",
    "    CREATE INDEX nome_do_index ON tabela(campo)\n",
    "\n",
    "Precisamos de uma autorizacao / nivel de acesso do tipo ALTER TABLE para criar índices\n",
    "\n",
    "Para avaliar se faz sentido criar um índice, podemos criar uma temp table igual a tabela de producao, criar os indices, rodar algumas queries em ambas as tabelas e comparar quanto tempo demora. Dessa forma conseguimos demonstrar que haverá ganho criando índices e ainda por cima com segurança\n",
    "\n",
    "Index criado manualmente ficam no grupo índices da tabela e podem ser deletados normalmente, exceto o index de primary key\n",
    "\n",
    "Quando usamos LIKE e '%', isso mata o index only scan e onera performance\n",
    "\n",
    "Normalmente quem insere o dado pode esperar mais do que quem quer ler o dado, pensa em um e-commerce"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Aula 13 - Partition\n",
    "\n",
    "- Criando uma tabela pessoas_partition\n",
    "- Adicionando 10 milhoes de registros\n",
    "- Criando index em first_name\n",
    "- Rodando query com first_name (index)\n",
    "    - SELECT COUNT(first_name) FROM pessoas_partition WHERE first_name = 'aa' -> Query complete 00:00:01.421 (Index only scan)\n",
    "    - SELECT COUNT(last_name) FROM pessoas_partition WHERE last_name = 'aa' -> Query complete 00:00:02.416 (seq scan)\n",
    "    - Lembrando que loops é o número de threads (cpus) utilizadas e dá para limitar max_parallel_workers = 0\n",
    "\n",
    "A ideia do partition é separar em particoes. no nosso exemplo divide em 5 tabelas com 2 milhoes de registros cada e aí vai buscar direto na tabela onde está o o valor procurado ao invés de procurar em todos os 10 milhoes\n",
    "\n",
    "Tipos de particionamento mais comuns:\n",
    "1. Por datas - cada tabela um range de data ex: ano (mais comum)\n",
    "2. Por valores / categorias (estados, países, ZIP Codes, etc) - cada tabela um estado ou país - tipo List (mais comum)\n",
    "3. Por ID (mais raro)\n",
    "\n",
    "Ver documentacao do Postgres - Table partitioning\n",
    "\n",
    "No nosso banco vamos particionar por:\n",
    "- Range dos números\n",
    "```sql\n",
    "     CREATE TABLE pessoas (\n",
    "         id SERIAL PRIMARY KEY,\n",
    "         first_name VARCHAR(3),\n",
    "         last_name VARCHAR(3),\n",
    "         estado VARCHAR(3)\n",
    "     ) PARTITION BY RANGE (id);\n",
    "\n",
    "     -- e depois\n",
    "\n",
    "    CREATE TABLE pessoas_part1 PARTITION OF pessoas FOR VALUES FROM (MINVALUE) TO (2000001);\n",
    "    CREATE TABLE pessoas_part2 PARTITION OF pessoas FOR VALUES FROM (2000001) TO (4000001);\n",
    "    CREATE TABLE pessoas_part3 PARTITION OF pessoas FOR VALUES FROM (4000001) TO (6000001);\n",
    "    CREATE TABLE pessoas_part4 PARTITION OF pessoas FOR VALUES FROM (6000001) TO (8000001);\n",
    "    CREATE TABLE pessoas_part5 PARTITION OF pessoas FOR VALUES FROM (8000001) TO (MAXVALUE);\n",
    "```\n",
    "Neste caso a particao 5 sempre ficara maior pois conforme meu indice for aumentando, vai cair td nela, entao teria que reparticionar regularmente, o que nao faz tanto sentido. Poderia ter um stored procedure para regularmente fazer esse rebalanceamento\n",
    "\n",
    "Neste caso se rodar um SELECT * FROM pessoas4 WHERE id = 7000000 ele vai mais rápido pq vai direto na part4\n",
    "\n",
    "- valores (estado). Neste caso caso temos que já criar a tabela com a coluna 'estado' como primary key para fazer o particionamento\n",
    "```sql\n",
    "CREATE TABLE pessoas (\n",
    "    id SERIAL,\n",
    "    first_name VARCHAR(3),\n",
    "    last_name VARCHAR(3),\n",
    "    estado VARCHAR(3),\n",
    "    PRIMARY KEY (id, estado)\n",
    ") PARTITION BY LIST (estado);\n",
    "\n",
    "-- e depois\n",
    "\n",
    "CREATE TABLE pessoas_sp PARTITION OF pessoas FOR VALUES IN ('SP');\n",
    "CREATE TABLE pessoas_rj PARTITION OF pessoas FOR VALUES IN ('RJ');\n",
    "CREATE TABLE pessoas_mg PARTITION OF pessoas FOR VALUES IN ('MG');\n",
    "CREATE TABLE pessoas_es PARTITION OF pessoas FOR VALUES IN ('ES');\n",
    "CREATE TABLE pessoas_df PARTITION OF pessoas FOR VALUES IN ('DF');\n",
    "```\n",
    "Neste caso se rodarmos um SELECT count(estado) from pessoas3 WHERE estado = 'RJ' será bem mais rápido pq está particionado, entao está procurando somente na tabela do RJ\n",
    "\n",
    "- Tempo da query sem particionar - Query complete 00:00:02.845\n",
    "- Tempo da query particionada e indexada por estado - Query complete 00:00:00.542\n",
    "\n",
    "PAra migrar uma tabela de producao que nao esteja particionada podemos fazer assim:\n",
    "1. Exportar o conteúdo da tabela de producao\n",
    "2. dropa tabela de producao\n",
    "3. cria a nova tabela particioanda sem index\n",
    "4. importa os dados de volta na nova tabela\n",
    "5. cria index global e index locais\n",
    "\n",
    "Sharding - quebrar as tabelas também pela regiao do servidor, ou seja a tabela dos EUA fica no servidor dos EUA, a da europa fica na europa, etc. A AWS faz isso. Particularmente util quando temos também imagems que sao pesadas\n",
    "\n",
    "No Sharding estamos pensando também em tempo de latencia"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Yes, it is possible to store image files in a PostgreSQL database. PostgreSQL provides several ways to handle images, and the best method depends on your specific requirements. Here's how you can do it:\n",
    "\n",
    "---\n",
    "\n",
    "### **Option 1: Use `BYTEA` to Store Images Directly in the Database**\n",
    "\n",
    "The `BYTEA` data type allows you to store binary data, such as image files, directly in the database.\n",
    "\n",
    "#### **Steps to Store Images Using `BYTEA`**\n",
    "\n",
    "1. **Create a Table**:\n",
    "   Create a table with a column of type `BYTEA` to store the image data.\n",
    "   ```sql\n",
    "   CREATE TABLE images (\n",
    "       id SERIAL PRIMARY KEY,\n",
    "       name TEXT NOT NULL,\n",
    "       data BYTEA NOT NULL\n",
    "   );\n",
    "   ```\n",
    "\n",
    "2. **Insert an Image**:\n",
    "   Use the `pg_read_binary_file` function or tools like `psql` to insert binary image data into the `BYTEA` column.\n",
    "\n",
    "   Example using `psql`:\n",
    "   ```sql\n",
    "   INSERT INTO images (name, data)\n",
    "   VALUES ('example_image', pg_read_binary_file('/path/to/image.jpg'));\n",
    "   ```\n",
    "\n",
    "3. **Retrieve the Image**:\n",
    "   Query the table to retrieve the image data and decode it as needed in your application.\n",
    "   ```sql\n",
    "   SELECT name, encode(data, 'base64') AS image_base64\n",
    "   FROM images;\n",
    "   ```\n",
    "\n",
    "4. **Decode the Image**:\n",
    "   - Use your application logic to decode the `base64` string back into binary data to render the image.\n",
    "\n",
    "---\n",
    "\n",
    "### **Option 2: Use Large Objects (LOBs)**\n",
    "\n",
    "PostgreSQL supports Large Objects (LOBs), which are suitable for storing large binary data such as images.\n",
    "\n",
    "#### **Steps to Store Images Using Large Objects**\n",
    "\n",
    "1. **Enable Large Objects**:\n",
    "   Ensure your database is set up to handle large objects (LOBs).\n",
    "\n",
    "2. **Create a Table**:\n",
    "   Create a table to reference the large object IDs.\n",
    "   ```sql\n",
    "   CREATE TABLE images (\n",
    "       id SERIAL PRIMARY KEY,\n",
    "       name TEXT NOT NULL,\n",
    "       lo_id OID -- Large Object ID\n",
    "   );\n",
    "   ```\n",
    "\n",
    "3. **Insert an Image**:\n",
    "   Use the `lo_import` function to load an image into a large object and store its OID in the table.\n",
    "   ```sql\n",
    "   INSERT INTO images (name, lo_id)\n",
    "   VALUES ('example_image', lo_import('/path/to/image.jpg'));\n",
    "   ```\n",
    "\n",
    "4. **Retrieve the Image**:\n",
    "   Use the `lo_export` function to extract the image.\n",
    "   ```sql\n",
    "   SELECT lo_export(lo_id, '/path/to/output/image.jpg')\n",
    "   FROM images\n",
    "   WHERE name = 'example_image';\n",
    "   ```\n",
    "\n",
    "---\n",
    "\n",
    "### **Option 3: Store Image Paths and Keep Files on Disk**\n",
    "\n",
    "This method involves storing only the file paths in the database while keeping the actual images on the file system.\n",
    "\n",
    "#### **Steps**\n",
    "\n",
    "1. **Create a Table**:\n",
    "   Create a table to store image metadata and file paths.\n",
    "   ```sql\n",
    "   CREATE TABLE images (\n",
    "       id SERIAL PRIMARY KEY,\n",
    "       name TEXT NOT NULL,\n",
    "       file_path TEXT NOT NULL\n",
    "   );\n",
    "   ```\n",
    "\n",
    "2. **Insert Image Paths**:\n",
    "   Insert metadata and file paths into the table.\n",
    "   ```sql\n",
    "   INSERT INTO images (name, file_path)\n",
    "   VALUES ('example_image', '/path/to/image.jpg');\n",
    "   ```\n",
    "\n",
    "3. **Access Images**:\n",
    "   Use your application to read and display the image from the file system using the stored file path.\n",
    "\n",
    "---\n",
    "\n",
    "### **Comparison of Methods**\n",
    "\n",
    "| **Method**              | **Advantages**                                                                 | **Disadvantages**                                                         |\n",
    "|-------------------------|-------------------------------------------------------------------------------|---------------------------------------------------------------------------|\n",
    "| **BYTEA**               | - Simple to implement.<br>- Full control over images within the database.     | - Large database size.<br>- May impact performance with large images.     |\n",
    "| **Large Objects (LOBs)**| - Efficient for large binary data.<br>- PostgreSQL optimizations for LOBs.    | - More complex to manage (special functions like `lo_import`).            |\n",
    "| **File Paths**           | - Database stays small.<br>- Easy to backup and manage files separately.      | - Relies on the file system.<br>- Risk of broken links if files are moved.|\n",
    "\n",
    "---\n",
    "\n",
    "### **Recommendations**\n",
    "\n",
    "1. **Use `BYTEA`**:\n",
    "   - If you want to store everything within the database.\n",
    "   - Suitable for small to medium-sized images.\n",
    "   \n",
    "2. **Use Large Objects**:\n",
    "   - If you are dealing with large images and need database-based management.\n",
    "\n",
    "3. **Use File Paths**:\n",
    "   - If you prioritize performance and prefer keeping the database size small.\n",
    "\n",
    "---\n",
    "\n",
    "Let me know which method you’d like to implement, and I can provide further assistance or examples tailored to your needs! 🚀"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Bootcamp aberto Modern Data Stack com SQL - Marc Lamberti\n",
    "\n",
    "## Parte 1A - com Marc\n",
    "\n",
    "Como Apache Airflow pode ajudar na rotina?\n",
    "1. Task scheduling and walkthroughs (tasks with multiple dependencies)\n",
    "2. Simplify management of multiple tasks\n",
    "3. Scalable\n",
    "4. Can integrate multiple tools\n",
    "5. Easy to migrate to Airflow if you have a python application using decorators\n",
    "\n",
    "Create projects that can match what you will see in the real world\n",
    "\n",
    "Stick to fundamentals - SQL, Python, data structures, check jobs and what companies are looking for in terms of tools and projects\n",
    "\n",
    "Steps to start\n",
    "1. install astro cli [astro cli install](https://www.astronomer.io/docs/astro/cli/install-cli/)\n",
    "2. astro dev init\n",
    "3. astro dev start - starts airflow locally\n",
    "4. Include folder can receive python scripts, SQL queries, requests, etc - makes dags easier to read/write\n",
    "5. setup airflow_settings.yaml file\n",
    "6. create your dag (data pipeline - tasks and dependencies within the tasks 0->1->2->3)\n",
    "\n",
    "To migrate stuff to airflow:\n",
    "1. put functions in 'include' folder\n",
    "2. import them on your dag\n",
    "3. add decorators to functions\n",
    "\n",
    "Orchestration - think of a cake recipe\n",
    "- recipe is the workflow (dag)\n",
    "- orchestrator is yourself (put the ingredients in the same order to make the cake)\n",
    "- orchestrator allows you to build pipelines to get the output as you want\n",
    "\n",
    "## Parte 1B - Hands on\n",
    "\n",
    "Airbyte - pode rodar localmente ou na sua cloud e ai fica de graça\n",
    "\n",
    "1. Setup sources (facebook, google analytics, excel)\n",
    "2. Setup destination (postgres @ render)\n",
    "3. Create connection\n",
    "    - select what to connect (campaigns, etc)\n",
    "    - schedule type: manual\n",
    "    - custom format - mandar para um schema chamado 'bronze'\n",
    "    - stream prefix 'facebook' p.e."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Bootcamp Aberto Modern Data Stack Parte 2\n",
    "\n",
    "CTRL+SHIFT+F formata a query do SQL no Dbeaver\n",
    "\n",
    "DBT faz abstracao do banco de dados\n",
    "\n",
    "1. Conecta ao banco de dados postgres\n",
    "2. Abre o IDE do dbt\n",
    "3. vai em models\n",
    "4. cria arquivos:\n",
    "    - sources.yml \n",
    "    - query1.sql\n",
    "    - query2.sql\n",
    "5. manda dbt.run\n",
    "\n",
    "pode colocar csvs e outros arquivos na pasta seed e ele já converte pra tabela no banco"
   ]
  }
 ],
 "metadata": {
  "language_info": {
   "name": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
